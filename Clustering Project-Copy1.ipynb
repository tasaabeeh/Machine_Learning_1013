{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35roXDEMudbw"
   },
   "source": [
    "# GUC Clustering Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIiItKbYudb2"
   },
   "source": [
    "**Objective:** \n",
    "The objective of this project teach students how to apply clustering to real data sets\n",
    "\n",
    "The projects aims to teach student: \n",
    "* Which clustering approach to use\n",
    "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
    "* How to tune the parameters of each data approach\n",
    "* What is the effect of different distance functions (optional) \n",
    "* How to evaluate clustering approachs \n",
    "* How to display the output\n",
    "* What is the effect of normalizing the data \n",
    "\n",
    "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MtHElDYdudb3"
   },
   "outputs": [],
   "source": [
    "# if plotnine is not installed in Jupter then use the following command to install it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RHS5ZoQudb4"
   },
   "source": [
    "Running this project require the following imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "QrueqJenudb5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.datasets import make_blobs\n",
    "from plotnine import *   \n",
    "# StandardScaler is a function to normalize the data \n",
    "# You may also check MinMaxScaler and MaxAbsScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster(X, km=[], num_clusters=0):\n",
    "    color = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'violet', 'aqua', 'pink']\n",
    "    alpha = 0.5\n",
    "    s = 20\n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=color[0], alpha=alpha, s=s)\n",
    "    else:\n",
    "        X_np = X.values if isinstance(X, pd.DataFrame) else X  # Convert to numpy array if X is a DataFrame\n",
    "        for i in range(num_clusters):\n",
    "            cluster_indices = np.where(km.labels_ == i)[0]\n",
    "            plt.scatter(X_np[cluster_indices, 0], X_np[cluster_indices, 1], c=color[i], alpha=alpha, s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0], km.cluster_centers_[i][1], c=color[i], marker='x', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZnIbT3Mudb6"
   },
   "source": [
    "## Multi Blob Data Set \n",
    "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
    "* Cluster the data set below using \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)\n",
    "display_cluster(Multi_blob_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDSIGjubudb8"
   },
   "source": [
    "### Kmeans \n",
    "* Use Kmeans with different values of K to cluster the above data \n",
    "* Display the outcome of each value of K \n",
    "* Plot distortion function versus K and choose the approriate value of k \n",
    "* Plot the silhouette_score versus K and use it to choose the best K \n",
    "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* using predefined kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with different values of K\n",
    "K_values = range(2, 10)  # Values of K to try\n",
    "distortions = []  # List to store distortion values\n",
    "silhouette_scores = []  # List to store silhouette scores\n",
    "\n",
    "for k in K_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(Multi_blob_Data)\n",
    "    distortions.append(km.inertia_)  # Sum of squared distances to closest cluster center\n",
    "    silhouette_scores.append(silhouette_score(Multi_blob_Data, km.labels_))  # Silhouette score\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(Multi_blob_Data, km, k)\n",
    "\n",
    "# Plot distortion function versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus K')\n",
    "\n",
    "# Plot silhouette score versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score versus K')\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = K_values[np.argmax(silhouette_scores)]\n",
    "print(\"Best value of K based on silhouette score:\", best_K)\n",
    "print(\"Best silhouette score:\",np.max(silhouette_scores) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE7dvpOAudb9"
   },
   "source": [
    "### Hierarchal Clustering\n",
    "* Use AgglomerativeClustering function to  to cluster the above data \n",
    "* In the  AgglomerativeClustering change the following parameters \n",
    "    * Affinity (use euclidean, manhattan and cosine)\n",
    "    * Linkage( use average and single )\n",
    "    * Distance_threshold (try different)\n",
    "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "3O_6WwKoudb-"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(Multi_blob_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from itertools import combinations\n",
    "\n",
    "def hierarchical_clustering(df):\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1  # Initialize with a value that ensures any calculated silhouette score will be better\n",
    "    best_params = None\n",
    "\n",
    "    affinities = ['euclidean', 'cityblock', 'cosine']\n",
    "    linkages = ['average', 'single']\n",
    "    distance_thresholds = [None]\n",
    "    numeric_distance_thresholds = [3, 7]  # Add numeric distance thresholds here\n",
    "\n",
    "    # Iterate over parameter combinations with numeric distance_thresholds\n",
    "    for affinity in affinities:\n",
    "        for linkage_method in linkages:\n",
    "            for distance_threshold in numeric_distance_thresholds:\n",
    "                # Perform Agglomerative Clustering\n",
    "                clustering = AgglomerativeClustering(affinity=affinity, linkage=linkage_method,\n",
    "                                                     distance_threshold=distance_threshold, n_clusters=None)\n",
    "                Z = linkage(df, method=linkage_method, metric=affinity)\n",
    "                cluster_labels = clustering.fit_predict(df)\n",
    "\n",
    "                # Check if multiple clusters are formed\n",
    "                if len(np.unique(cluster_labels)) > 1:\n",
    "                    # Calculate silhouette score\n",
    "                    silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "                    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "                    # Plot dendrogram\n",
    "                    plt.figure(figsize=(10, 10))\n",
    "                    plt.title(f'Dendrogram - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}')\n",
    "                    dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
    "                    plt.xlabel('Sample Index')\n",
    "                    plt.ylabel('Distance')\n",
    "                    plt.show()\n",
    "\n",
    "                    # Plot resulting clusters\n",
    "                    for pair in combinations(range(df.shape[1]), 2):\n",
    "                        plt.figure(figsize=(8, 6))\n",
    "                        plt.scatter(df.iloc[:, pair[0]], df.iloc[:, pair[1]], c=cluster_labels, cmap='viridis')\n",
    "                        plt.title(f\"Clusters - Features {pair[0]} and {pair[1]} - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}\")\n",
    "                        plt.xlabel(f'Feature {pair[0]}')\n",
    "                        plt.ylabel(f'Feature {pair[1]}')\n",
    "                        plt.colorbar(label='Cluster')\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "\n",
    "                    # Check if this silhouette score is better than the current best\n",
    "                    if silhouette_avg > best_silhouette_score:\n",
    "                        best_silhouette_score = silhouette_avg\n",
    "                        best_params = {'Affinity': affinity, 'Linkage': linkage_method, 'Distance Threshold': distance_threshold}\n",
    "\n",
    "    # Iterate over parameter combinations with distance_thresholds\n",
    "    for affinity in affinities:\n",
    "        for linkage_method in linkages:\n",
    "            for distance_threshold in distance_thresholds:\n",
    "                # Set n_clusters to 2 if distance_threshold is None\n",
    "                n_clusters = 2 if distance_threshold is None else None\n",
    "                # Perform Agglomerative Clustering\n",
    "                clustering = AgglomerativeClustering(affinity=affinity, linkage=linkage_method,\n",
    "                                                     distance_threshold=distance_threshold, n_clusters=n_clusters)\n",
    "                cluster_labels = clustering.fit_predict(df)\n",
    "                Z = linkage(df, method=linkage_method, metric=affinity)\n",
    "\n",
    "                # Check if multiple clusters are formed\n",
    "                if len(np.unique(cluster_labels)) > 1:\n",
    "                    # Calculate silhouette score\n",
    "                    silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "                    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "                    # Plot dendrogram\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.title(f'Dendrogram - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}')\n",
    "                    dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
    "                    plt.xlabel('Sample Index')\n",
    "                    plt.ylabel('Distance')\n",
    "                    plt.show()\n",
    "\n",
    "                    # Plot resulting clusters\n",
    "                    for pair in combinations(range(df.shape[1]), 2):\n",
    "                        plt.figure(figsize=(8, 6))\n",
    "                        plt.scatter(df.iloc[:, pair[0]], df.iloc[:, pair[1]], c=cluster_labels, cmap='viridis')\n",
    "                        plt.title(f\"Clusters - Features {pair[0]} and {pair[1]} - Affinity: {affinity}, Linkage: {linkage_method}, Distance Threshold: {distance_threshold}\")\n",
    "                        plt.xlabel(f'Feature {pair[0]}')\n",
    "                        plt.ylabel(f'Feature {pair[1]}')\n",
    "                        plt.colorbar(label='Cluster')\n",
    "                        plt.grid(True)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "\n",
    "                    # Check if this silhouette score is better than the current best\n",
    "                    if silhouette_avg > best_silhouette_score:\n",
    "                        best_silhouette_score = silhouette_avg\n",
    "                        best_params = {'Affinity': affinity, 'Linkage': linkage_method, 'Distance Threshold': distance_threshold}\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJE7vQKudb-"
   },
   "source": [
    "### DBScan\n",
    "* Use DBScan function to  to cluster the above data \n",
    "* In the  DBscan change the following parameters \n",
    "    * EPS (from 0.1 to 3)\n",
    "    * Min_samples (from 5 to 25)\n",
    "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
    "* Plot the resulting Clusters in this case \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observations and comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DBSCAN_clustering(df):\n",
    "    # Define the range of parameter values to experiment with\n",
    "    eps_values = np.linspace(0.1, 3, 30)  # Range of eps values from 0.1 to 3\n",
    "    min_samples_values = range(5, 26)  # Range of min_samples values from 5 to 25\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1  # Initialize with a value that ensures any calculated silhouette score will be better\n",
    "    best_params = None\n",
    "    best_cluster_labels = None\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Iterate over parameter combinations\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            # Perform DBSCAN clustering\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            cluster_labels = dbscan.fit_predict(df)\n",
    "\n",
    "            # Check if only one unique label is detected\n",
    "            if len(np.unique(cluster_labels)) <= 1:\n",
    "                continue\n",
    "\n",
    "            # Calculate silhouette score\n",
    "            silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "\n",
    "            # Store silhouette score and parameters\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "            # Check if this silhouette score is better than the current best\n",
    "            if silhouette_avg > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette_avg\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "                best_cluster_labels = cluster_labels\n",
    "\n",
    "            # Plot the resulting clusters for each pair of features\n",
    "            n_features = df.shape[1]\n",
    "            for i in range(n_features):\n",
    "                for j in range(i+1,n_features):\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.scatter(df.iloc[:, i], df.iloc[:, j], c=cluster_labels, cmap='viridis', s=50, alpha=0.5)\n",
    "                    plt.xlabel(df.columns[i])\n",
    "                    plt.ylabel(df.columns[j])\n",
    "                    plt.colorbar(label='Cluster')\n",
    "                    plt.grid(True)\n",
    "                    plt.tight_layout()\n",
    "                    plt.axis\n",
    "                    plt.show()\n",
    "\n",
    "    # Plot silhouette score versus eps and min_samples\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(silhouette_scores)), silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title('Silhouette Score vs. Parameters (EPS and Min Samples)')\n",
    "    plt.xlabel('Parameter Combination')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_clustering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "source": [
    "### Gaussian Mixture\n",
    "* Use GaussianMixture function to cluster the above data \n",
    "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
    "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def GMMClustering(df):\n",
    "    # Define covariance types to test\n",
    "    covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1\n",
    "    best_params = {}\n",
    "\n",
    "    # Initialize lists to store silhouette scores and corresponding parameters\n",
    "    silhouette_scores = []\n",
    "    parameters = []\n",
    "\n",
    "    # Iterate over covariance types\n",
    "    for covariance_type in covariance_types:\n",
    "        # Perform clustering\n",
    "        gmm = GaussianMixture(n_components=3, covariance_type=covariance_type)\n",
    "        cluster_labels = gmm.fit_predict(df)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "\n",
    "        # Store silhouette score and parameters\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        parameters.append({'Covariance Type': covariance_type})\n",
    "\n",
    "        # Check if this silhouette score is better than the current best\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_params = {'Covariance Type': covariance_type}\n",
    "            best_cluster_labels = cluster_labels\n",
    "\n",
    "    # Plot scatter plot for each pair of features\n",
    "    n_features = df.shape[1]\n",
    "    for i in range(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            # Scatter plot\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(df.iloc[:, i], df.iloc[:, j], c=best_cluster_labels, cmap='viridis', s=50, alpha=0.5)\n",
    "            plt.xlabel(f'Feature {i}')\n",
    "            plt.ylabel(f'Feature {j}')\n",
    "            plt.title('Scatter Plot')\n",
    "\n",
    "            # Fit GaussianMixture model for the current pair of features\n",
    "            gmm_pair = GaussianMixture(n_components=3, covariance_type=best_params['Covariance Type'])\n",
    "            X_pair = df[[df.columns[i], df.columns[j]]]\n",
    "            gmm_pair.fit(X_pair)\n",
    "\n",
    "            # Contour plot for Gaussian mixture\n",
    "            plt.subplot(1, 2, 2)\n",
    "            x_min, x_max = df.iloc[:, i].min() - 1, df.iloc[:, i].max() + 1\n",
    "            y_min, y_max = df.iloc[:, j].min() - 1, df.iloc[:, j].max() + 1\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                                 np.linspace(y_min, y_max, 100))\n",
    "            Z = gmm_pair.score_samples(np.column_stack([xx.ravel(), yy.ravel()]))\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            plt.contourf(xx, yy, Z, cmap='viridis', levels=20, alpha=0.5)\n",
    "            plt.xlabel(f'Feature {i}')\n",
    "            plt.ylabel(f'Feature {j}')\n",
    "            plt.title('Contour Plot')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Plot silhouette score versus covariance type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(silhouette_scores)), silhouette_scores, tick_label=[param['Covariance Type'] for param in parameters])\n",
    "    plt.title('Silhouette Score versus Covariance Type')\n",
    "    plt.xlabel('Covariance Type')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMMClustering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92lZkkyudb_"
   },
   "source": [
    "## iris data set \n",
    "The iris data set is test data set that is part of the Sklearn module \n",
    "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
    "\n",
    "The data represents three classes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repeat all the above clustering approaches and steps on the above data \n",
    "* Normalize the data then repeat all the above steps \n",
    "* Compare between the different clustering approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "_QaCWyyCudcA",
    "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "iris_data.target[[10, 25, 50]]\n",
    "#array([0, 0, 1])\n",
    "list(iris_data.target_names)\n",
    "['setosa', 'versicolor', 'virginica']\n",
    "iris_df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kmeans using predefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with different values of K\n",
    "K_values = range(2, 10)  # Values of K to try\n",
    "distortions = []  # List to store distortion values\n",
    "silhouette_scores = []  # List to store silhouette scores\n",
    "\n",
    "for k in K_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(iris_df)\n",
    "    distortions.append(km.inertia_)  # Sum of squared distances to closest cluster center\n",
    "    silhouette_scores.append(silhouette_score(iris_df, km.labels_))  # Silhouette score\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(iris_df, km, k)\n",
    "\n",
    "# Plot distortion function versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus K')\n",
    "\n",
    "# Plot silhouette score versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score versus K')\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = K_values[np.argmax(silhouette_scores)]\n",
    "print(\"Best value of K based on silhouette score:\", best_K)\n",
    "print(\"Best silhouette score:\",np.max(silhouette_scores) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DBSCAN clusteing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_clustering(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMM gaussian clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMMClustering(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after normalizing iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "iris_normalized = scaler.fit_transform(iris_df)\n",
    "iris_normalized_df = pd.DataFrame(iris_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kmeans using predefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with different values of K\n",
    "K_values = range(2, 10)  # Values of K to try\n",
    "distortions = []  # List to store distortion values\n",
    "silhouette_scores = []  # List to store silhouette scores\n",
    "\n",
    "for k in K_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(iris_normalized_df)\n",
    "    distortions.append(km.inertia_)  # Sum of squared distances to closest cluster center\n",
    "    silhouette_scores.append(silhouette_score(iris_normalized_df, km.labels_))  # Silhouette score\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(iris_normalized_df, km, k)\n",
    "\n",
    "# Plot distortion function versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus K')\n",
    "\n",
    "# Plot silhouette score versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score versus K')\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = K_values[np.argmax(silhouette_scores)]\n",
    "print(\"Best value of K based on silhouette score:\", best_K)\n",
    "print(\"Best silhouette score:\",np.max(silhouette_scores) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering(iris_normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_clustering(iris_normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMMClustering(iris_normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare between the different clustering approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for the distance being euclidean in the kmeans, we can notice that the best k(@ iris_df)= 5 and (@ iris_normalized_df)=3\n",
    "* while for the distance being pearson in the kmeans, we can notice that the best k(@ iris_df)= 2 and (@ iris_normalized_df)=2\n",
    "* for the DBSCAN clustering, we can notice that there was a slight change in the values of the silhouette score and an 0.4999 difference in the epsilon values.\n",
    "* for the hierarchical clustering, we can notice that the silhouette score changed slightly and the distance threshold changed from 3 to None\n",
    "* for the GMM gaussian, we can notice that the silhouette score changed slightly and the covariance type changed from spherical to diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2oBmWT2udcA"
   },
   "source": [
    "## Customer dataset\n",
    "Repeat all the above on the customer data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/tasabeehzainyahoo.com/Desktop/Machine Learning/Assignments/Assignment 1/data/\"\n",
    "customer_df= pd.read_csv(data_dir + \"Customer data.csv\")\n",
    "customer_df.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kmeans using predefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with different values of K\n",
    "K_values = range(2, 10)  # Values of K to try\n",
    "distortions = []  # List to store distortion values\n",
    "silhouette_scores = []  # List to store silhouette scores\n",
    "\n",
    "for k in K_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(customer_df)\n",
    "    distortions.append(km.inertia_)  # Sum of squared distances to closest cluster center\n",
    "    silhouette_scores.append(silhouette_score(customer_df, km.labels_))  # Silhouette score\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(customer_df, km, k)\n",
    "\n",
    "# Plot distortion function versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus K')\n",
    "\n",
    "# Plot silhouette score versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score versus K')\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = K_values[np.argmax(silhouette_scores)]\n",
    "print(\"Best value of K based on silhouette score:\", best_K)\n",
    "print(\"Best silhouette score:\",np.max(silhouette_scores) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_clustering(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMM Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMMClustering(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after normalizing customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_customer = MinMaxScaler()\n",
    "customer_normalized = scaler_customer.fit_transform(customer_df)\n",
    "customer_normalized_df = pd.DataFrame(customer_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kmeans using predefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering with different values of K\n",
    "K_values = range(2, 10)  # Values of K to try\n",
    "distortions = []  # List to store distortion values\n",
    "silhouette_scores = []  # List to store silhouette scores\n",
    "\n",
    "for k in K_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(customer_normalized_df)\n",
    "    distortions.append(km.inertia_)  # Sum of squared distances to closest cluster center\n",
    "    silhouette_scores.append(silhouette_score(customer_normalized_df, km.labels_))  # Silhouette score\n",
    "\n",
    "    # Display clusters for each value of K\n",
    "    plt.figure()\n",
    "    plt.title(\"K-means clustering with K={}\".format(k))\n",
    "    display_cluster(customer_normalized_df, km, k)\n",
    "\n",
    "# Plot distortion function versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion versus K')\n",
    "\n",
    "# Plot silhouette score versus K\n",
    "plt.figure()\n",
    "plt.plot(K_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score versus K')\n",
    "\n",
    "# Find the best value of K based on silhouette score\n",
    "best_K = K_values[np.argmax(silhouette_scores)]\n",
    "print(\"Best value of K based on silhouette score:\", best_K)\n",
    "print(\"Best silhouette score:\",np.max(silhouette_scores) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_clustering(customer_normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN_clustering(customer_normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GMM gaussian clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMMClustering(customer_normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing between normalized and normal customer df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can notice that in the DBSCAN, when the data was not normalized,the DBSCAN did not work as the features with large scale would dominate the clustering process.\n",
    "* so when we normalized, the features with large scale were transformed to be close to each other , hence the DBSCAN worked."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
